{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO8OJ1PTscwGJxvubOh7fQB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 판례 일련 번호 불러오기"],"metadata":{"id":"MUzytBhmj4js"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"j6sWq1-Uik43"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","import urllib.request as REQ\n","from xml.etree.ElementTree import parse\n","from urllib import parse\n","import random\n","import json\n","\n","u1 = ('http://www.law.go.kr/DRF/lawSearch.do?OC=ID&search=2&target=prec&type=XML&display=100&page=')\n","u2 = ('http://www.law.go.kr/DRF/lawService.do?OC=ID&type=XML&target=prec&ID=')\n","wordList = ['친족', '면접교섭권', '약혼', '부양', '양육비', '이혼', '혼인', '위자료', '재산분할청구권', '입양', '파양', '친양자', '후견인']\n","numList = []\n","\n","#판례 제목에 wordList의 단어가 들어간 판례 목록을 불러와 판례일련번호를 titleDic에 저장\n","for i in range(len(wordList)):\n","  t = 1\n","  w = parse.quote(wordList[i])\n","  while True:\n","    url = u1 + str(t) + '&query=' + w\n","    try:\n","      xml = REQ.urlopen(url).read()\n","      soup = BeautifulSoup(xml, \"lxml-xml\")\n","      nums = soup.select('판례일련번호')\n","      kinds = soup.select('사건종류명')\n","      if len(nums) == 0:\n","        break\n","      for k in range(len(nums)):\n","          if kinds[k].text == '민사':\n","                numList.append(nums[k].text)\n","    except:\n","      print('오류')\n","    t += 1\n","\n","new_file = open('/content/drive/MyDrive/파일위치/numList_6.txt', \"w\", encoding='utf-8')\n","for n in numList:\n","  new_file.write(n+'\\n')\n","new_file.close()\n"]},{"cell_type":"code","source":["import random\n","import pandas as pd\n","\n","#랜덤하게 8천 개 뽑기\n","\n","f = open('/content/drive/MyDrive/파일위치/numList_6_random.txt', 'r')\n","s = f.read()\n","numList = s.split('\\n')\n","random.shuffle(numList)\n","\n","pan = pd.read_pickle(\"/content/drive/MyDrive/파일위치/pan_6_random_1000.pkl\")\n","numList2 = list(pan['number'])\n","\n","for n in numList:\n","  if n in numList2:\n","    continue\n","  else:\n","    numList2.append(n)\n","  if len(numList2) == 8000:\n","    break\n"],"metadata":{"id":"eq2pS7GdiuMH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 판례 본문 저장"],"metadata":{"id":"AA49MXPNj_HG"}},{"cell_type":"code","source":["  from bs4 import BeautifulSoup\n","  import urllib.request as REQ\n","  from xml.etree.ElementTree import parse\n","  from urllib import parse\n","  import random\n","  import json\n","  from tqdm import tqdm\n","  u1 = ('http://www.law.go.kr/DRF/lawSearch.do?OC=ID&search=2&target=prec&type=XML&display=100&page=')\n","  u2 = ('http://www.law.go.kr/DRF/lawService.do?OC=ID&type=XML&target=prec&ID=')\n","  pansi_list = [] #판시사항 리스트\n","  art_list = [] #참조조문 리스트\n","  num_list = [] #일련번호 리스트\n","  panyo_list = [] #판결요지 리스트\n","  pan_list = [] #판례 내용(본문) 리스트\n","  precedentDic = {} #데이터 프레임으로 만들기 위한 딕셔너리\n","\n","  for i in tqdm(range(len(numList2))):\n","    try:\n","                n = parse.quote(str(numList2[i]))\n","                url2 = u2 + n\n","                xml = REQ.urlopen(url2).read()\n","                soup = BeautifulSoup(xml, \"lxml-xml\")\n","                res = soup.find('판시사항').text\n","                res2 = soup.find('판결요지').text\n","                jo = soup.find('참조조문').text\n","                if res != ' ' and res2 != ' ':\n","                  num_list.append(str(numList2[i]))\n","                  pansi_list.append(res)\n","                  panyo_list.append(res2)\n","                    pan_list.append(soup.find('판례내용').text)\n","                else:\n","                  continue\n","                if jo != ' ':\n","                  art_list.append(jo)\n","                else:\n","                  art_list.append('')\n","                n_list.append(n)\n","    except Exception as e:\n","                print(e)\n","    if i % 100 == 99:\n","      precedentDic['number'] = num_list\n","      precedentDic['pansi'] = pansi_list\n","      precedentDic['panyo'] = panyo_list\n","      precedentDic['pan'] = panyo_list\n","      precedentDic['jo'] = art_list\n","      p_obj = pd.DataFrame(precedentDic, columns = ['number', 'pansi', 'panyo', 'jo'])\n","      p_obj.to_pickle(\"/content/drive/MyDrive/파일위치/pan_6_random_8000.pkl\")\n","precedentDic['number'] = num_list\n","precedentDic['pansi'] = pansi_list\n","precedentDic['panyo'] = panyo_list\n","precedentDic['jo'] = art_list\n","p_obj = pd.DataFrame(precedentDic, columns = ['number', 'pansi', 'panyo', 'jo'])\n","p_obj.to_pickle(\"/content/drive/MyDrive/파일위치/pan_6_random_8000.pkl\")\n"],"metadata":{"id":"r8dlTYrjjSvg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 참조 조문에 민법이 있는 것만 저장"],"metadata":{"id":"6k1UjjgFkDZd"}},{"cell_type":"code","source":["jo_list = [] #각 판례 별로 참조 조문 리스트 저장(2차원)\n","for i in range(len(p_obj)):\n","  p = p_obj['jo'][i].split(',') #참조 조문 분리-[‘민법 제1조’, ‘민사소송법 제1조’] 이런 형태\n","  for p in p:\n","    for w in p.split():\n","      if not w.isalnum() and w.find('법') == -1:\n","        continue\n","    jo_list.append(w) #법 이름 저장\n","for i in range(len(p_obj)):\n","  jo = jo_list[i]\n","  c = 0 #민법이 있으면 0, 없으면 1\n","  for j in jo:\n","    if j == '민법':\n","      c = 1\n","       break\n","  if c == 0: #민법이 없으면 삭제\n","    p_obj = p_obj.drop(i)\n","p_obj.to_pickle(\"/content/drive/MyDrive/law/pan_6_random_8000.pkl\")\n"],"metadata":{"id":"210o-4VPjtUa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 민법 조항 저장"],"metadata":{"id":"fGX9RCI3u0Hb"}},{"cell_type":"code","source":["import pandas as pd\n","f = open('/content/drive/MyDrive/파일위치/민법.txt', 'r')\n","s = f.read()\n","s = s.split('\\n')  #민법 전체 텍스트를 한 줄 씩 분리한 리스트\n","jo = 1 #조항 번호(제 _조)\n","n_list = [] #조항 제목 리스트\n","a_list=[]#조항 내용 리스트\n","al = [] #각 조항 내용의 문장 리스트\n","check = 0 #해당 조항에 검색 단어 있으면 1, 없으면 0\n","for i in range(len(s)):\n","            #편이나 절, 장 등은 제외함. (삭제된 것들도)--제4편의 제2장이 너무 길어서 따로 조건에 추가함\n","            if (len(s[i]) < 12 and s[i].find('(') == -1 and s[i].find('.') == -1) or s[i].find('제2장') != -1 or s[i].find('제5편') != -1:\n","                #삭제된 조항이면 jo 증가\n","                if s[i].find('문헌') != -1:\n","                  if len(al) != 0:\n","                    a_list.append(al)\n","                  jo += 1\n","                  al = []\n","                continue\n","            if s[i].find('제1장') != -1 or s[i].find('제1절') != -1:\n","              continue\n","            str2 = '제' + str(jo+1) + '조'\n","            if s[i].find(str2) == 0 and s[i].find('(') != -1: #제(jo+1)조\n","                jo +=1\n","            str1 = '제' + str(jo) + '조'\n","            if s[i].find(str1) == 0 and s[i].find('(') != -1: ##제(jo)조\n","                if len(al) != 0:\n","                  a_list.append(al)\n","                  al = []\n","                n_list.append(s[i]) #조항 제목 추가\n","            else:\n","                al.append(s[i]) #조항 내용 추가\n","a_list.append(al)\n","aDic = {}\n","aDic['number'] = n_list\n","aDic['contents'] = a_list\n","a_obj = pd.DataFrame(aDic, columns = ['number', 'contents'])\n","a_obj.to_pickle(\"/content/drive/MyDrive/파일위치/article.pkl\")\n"],"metadata":{"id":"9N3Kk2tSu4Ml"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Kobert 학습 데이터_법률 구조 공단 상담 데이터"],"metadata":{"id":"szZl8pdonMHW"}},{"cell_type":"code","source":["import urllib.request as REQ\n","from bs4 import BeautifulSoup\n","from urllib import parse\n","import pandas as pd\n","\n","u = \"https://www.klac.or.kr/legalinfo/counselView.do?pageIndex=1&folderId=006&caseId=case-\"\n","numList = ['004', '005', '006', '007', '008', '011', '012'] #각각 손해배상, 민사일반, 물권, 채권, 친족, 계약, 상속\n","sentList1 = [] #제목 리스트(질문이 짧게 요약된 형태, 제목도 bert 학습 시 사용)\n","sentList2 = [] #질문 리스트\n","labelList = [] #레이블 리스트 손해배상이면 0, 민사일반이면 1 … 상속이면 6\n","df = pd.DataFrame(columns=['title', 'question', 'answer', 'label'])\n","for n in range(0, len(numList)):\n","  i = 1\n","  while True:\n","    num = str(i)\n","    for k in range(5-len(num)):\n","      num = '0' + num\n","    url = u + numList[n] + '-' + num\n","    print(url)\n","    try:\n","      html = REQ.urlopen(url).read()\n","      soup = BeautifulSoup(html, \"html.parser\")\n","      try:\n","        wList1 = soup.select('dd')\n","        if len(wList1[4].text) == 0:\n","          break\n","        sentList1.append(wList1[4].text) #제목 추가\n","        sentList2.append(wList1[5].text) #질문 추가\n","        labelList.append(n)\n","      except:\n","        print('오류')\n","    except:\n","      print('오류')\n","    i += 1\n","qDic = {}\n","qDic['title'] = sentList1\n","qDic['question'] = sentList2\n","qDic['label'] = labelList\n","df = pd.DataFrame(qDic, columns = ['title', 'question', 'label'])\n","df.index = range(len(df))\n","df.to_pickle(\"/content/drive/MyDrive/law/q_7400.pkl\")\n"],"metadata":{"id":"_yGjyoK5nLBP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Kobart 학습 데이터_요약문 및 레포터 생성 데이터"],"metadata":{"id":"2Ybc-j_NmNlN"}},{"cell_type":"code","source":["import json\n","import os\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","path = '../Downloads/022.요약문 및 레포트 생성 데이터/01.데이터/1.Training/라벨링데이터/TL1/'\n","folder_list = os.listdir(path)\n","sentence_list = [] #원문 리스트\n","summary_list = [] #요약문 리스트\n","for f in folder_list:\n","    file_path = path + f + '/20per'\n","    file_list = os.listdir(file_path)\n","    for file in file_list:\n","        file = file_path + '/' + file\n","        with open(file, 'r', encoding='UTF8') as file:\n","            print('읽기 완료')\n","            data = json.load(file)\n","            sentence_list.append(data['Meta(Refine)']['passage'])\n","            summary_list.append(data['Annotation']['summary1'])\n","new_df = {}\n","new_df['news'] = sentence_list\n","new_df['summary'] = summary_list\n","p_obj = pd.DataFrame(new_df, columns = ['news', 'summary'])\n","p_obj\n","#학습 데이터, 검증 데이터로 나누기\n","data_train, dataset_val = train_test_split(p_obj, test_size=0.2, random_state=0)\n","dataset_ilst = dataset_ilst.sort_index()\n","dataset_ilst = dataset_ilst.reset_index(drop=True)\n","dataset_ilst = dataset_ilst[:60000] #너무 많아서 6만 개만 저장\n","dataset_ilst.to_csv('train.tsv', index=False, sep=\"\\t\")\n","dataset_val = dataset_val.sort_index()\n","dataset_val = dataset_val.reset_index(drop=True)\n","dataset_val = dataset_val[:20000] #너무 많아서 2만 개만 저장\n","dataset_val.to_csv('test.tsv', index=False, sep=\"\\t\")\n"],"metadata":{"id":"9BU07f_PmRgI"},"execution_count":null,"outputs":[]}]}